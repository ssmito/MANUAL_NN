{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_sum:\n",
    "    def __init__(self, dim=0):\n",
    "        self.dim=dim\n",
    "    def __call__(self, x):\n",
    "        self.out = x.sum(dim=self.dim, keepdims=True)\n",
    "        self.x=x\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.grad= torch.ones_like(self.x)\n",
    "        return self.grad\n",
    "\n",
    "class simple_mul:\n",
    "    def __init__(self):\n",
    "        self.scalar=torch.rand(1, requires_grad=True)\n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out = x*self.scalar\n",
    "        return self.out \n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.scalar_grad= torch.sum(self.x)\n",
    "            self.grad=torch.ones_like(self.x)*self.scalar\n",
    "        return self.grad   \n",
    "\n",
    "class simple_relu:\n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out= torch.maximum(x, torch.zeros_like(x))\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            mask=self.x > 0 #alternative mask -> self.out > 0 but in any case if self.x > 0 then self.out > 0 and self.out <= 0 otherwise so both options should lead to the same mask\n",
    "            self.grad=mask*1.0\n",
    "        return self.grad\n",
    "\n",
    "class simple_linear:\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        self.w = torch.randn(fan_in, fan_out, requires_grad=True)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out= x @ self.w\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.w_grad=self.x\n",
    "            self.grad=self.w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1688, -1.5060], requires_grad=True)\n",
      "out tensor([-1.3372], grad_fn=<SumBackward1>)\n",
      "x.grad None\n",
      "out.grad None\n",
      "x.grad tensor([1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_983/2354017059.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print('out.grad',out.grad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simp=simple_sum()\n",
    "x=torch.randn(2)\n",
    "x.requires_grad=True\n",
    "print(x)\n",
    "out=simp(x)\n",
    "print('out',out)\n",
    "print('x.grad',x.grad)\n",
    "out.backward()\n",
    "\n",
    "print('out.grad',out.grad)\n",
    "print('x.grad',x.grad)\n",
    "\n",
    "simp.back(),simp.x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4099, 1.5743], requires_grad=True)\n",
      "outm tensor([0.3180, 1.2213], grad_fn=<MulBackward0>)\n",
      "y.grad None\n",
      "y.grad tensor([0.7758, 0.7758])\n"
     ]
    }
   ],
   "source": [
    "simpm=simple_mul()\n",
    "y=torch.randn(2)\n",
    "y.requires_grad=True\n",
    "print(y)\n",
    "outm=simpm(y)\n",
    "print('outm',outm)\n",
    "print('y.grad',y.grad)\n",
    "outm_sum=outm.sum(0)\n",
    "outm_sum.backward()\n",
    "# print('outm.grad',outm.grad)\n",
    "print('y.grad',y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7758], requires_grad=True),\n",
       " tensor([0.7758, 0.7758]),\n",
       " tensor([0.7758, 0.7758]),\n",
       " tensor([1.9842]),\n",
       " tensor(1.9842))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpm.scalar, simpm.back(),simpm.x.grad, simpm.scalar.grad, simpm.scalar_grad,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[-0.0265, -1.7565, -0.5568],\n",
      "        [ 1.8134,  0.1513, -0.2631],\n",
      "        [-0.7829, -0.9893, -2.1119]], requires_grad=True) \n",
      "k: tensor([[ 1.0040, -2.5946, -2.9318]], grad_fn=<SumBackward1>) \n",
      "l: tensor([[ 0.4635, -1.1978, -1.3535]], grad_fn=<MulBackward0>) \n",
      "m: tensor(-2.0877, grad_fn=<SumBackward0>) \n",
      "mul1.scalar: tensor([0.4616], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "sum1=simple_sum(0)\n",
    "mul1=simple_mul()\n",
    "j=torch.randn(3,3, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=sum1(j)\n",
    "k.retain_grad()\n",
    "l=mul1(k)\n",
    "l.retain_grad()\n",
    "m=torch.sum(l)\n",
    "m.retain_grad()\n",
    "m.backward()\n",
    "print('j:',j, '\\nk:', k , '\\nl:',l,'\\nm:', m,'\\nmul1.scalar:', mul1.scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul1.back() tensor([[0.4616, 0.4616, 0.4616]]) \n",
      "mul1.scalar_grad tensor(-4.5224) \n",
      "mul1.scalar.grad tensor([-4.5224])\n",
      "sum1.back() tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "j.grad tensor([[0.4616, 0.4616, 0.4616],\n",
      "        [0.4616, 0.4616, 0.4616],\n",
      "        [0.4616, 0.4616, 0.4616]]) torch.Size([3, 3]) \n",
      "k.grad tensor([[0.4616, 0.4616, 0.4616]]) torch.Size([1, 3]) \n",
      "l.grad tensor([[1., 1., 1.]]) torch.Size([1, 3]) \n",
      "m.grad tensor(1.) torch.Size([])\n",
      "*************** calculated with .back() in each layer********************\n",
      "dm_dl*dl_dk = dm_dl * mul1.back() \n",
      "back tensor([[0.4616, 0.4616, 0.4616]]) back.shape torch.Size([1, 3])\n",
      "dm_dj = dm_dl*dl_dk*dk_dj = dm_dl * sum1.back() * mul1.back()\n",
      "back tensor([[0.4616, 0.4616, 0.4616],\n",
      "        [0.4616, 0.4616, 0.4616],\n",
      "        [0.4616, 0.4616, 0.4616]]) back.shape torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# dm_dj = dm_dl*dl_dk*dk_dj\n",
    "#dm_dl -> sum ->  [1, 1]\n",
    "#dl_dk -> mul1 -> [0.5502, 0.5502]\n",
    "#dk_dj -> sum1 -> [[1., 1.],\n",
    "#                  [1., 1.]]\n",
    "#dm_dj = [[0.5502, 0.5502],\n",
    "#         [0.5502, 0.5502]]\n",
    "\n",
    "print('mul1.back()',mul1.back(), '\\nmul1.scalar_grad', mul1.scalar_grad, '\\nmul1.scalar.grad', mul1.scalar.grad)\n",
    "print('sum1.back()',sum1.back())\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nm.grad',m.grad, m.shape)\n",
    "print('*************** calculated with .back() in each layer********************')\n",
    "mul1_back=mul1.back()\n",
    "sum1_back=sum1.back()\n",
    "print('dm_dl*dl_dk = dm_dl * mul1.back() ')\n",
    "back = torch.ones_like(l.grad) * mul1_back \n",
    "print('back',back, 'back.shape',back.shape)\n",
    "back = back *sum1_back\n",
    "print('dm_dj = dm_dl*dl_dk*dk_dj = dm_dl * sum1.back() * mul1.back()')\n",
    "print('back',back, 'back.shape',back.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000001336 5.000000000002558 1.0000000000012221\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "C = 4\n",
    "D = 6\n",
    "h=0.001\n",
    "A  = B * C + D\n",
    "Adb = (B+h) * C + D\n",
    "Adc = B * (C+h) + D\n",
    "Add = B * C + (D+h)\n",
    "da_db= (Adb-A)/h\n",
    "da_dc= (Adc-A)/h\n",
    "da_dd= (Add-A)/h\n",
    "\n",
    "print(da_db, da_dc, da_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " da_db  6.000000000000227 \n",
      " da_dc 5.000000000002558 \n",
      " da_dd 0.9999999999976694 \n",
      " dc_de 3.0000000000001137 \n",
      " dc_df 1.9999999999997797 \n",
      " da_de 15.000000000000568 \n",
      " da_df 9.99999999999801\n",
      " da_df = da_dc x dc_df -> 9.99999999999801 = 5.000000000002558 x 1.9999999999997797 \n",
      " da_de = da_dc x dc_de -> 15.000000000000568 = 5.000000000002558 x 3.0000000000001137\n",
      "COMPOSE FROM VARS\n",
      " da_df =  Da_dc x Dc_df -> 10 = 5 x 2 \n",
      " da_de = Da_dc x Dc_de -> 15 = 5 x 3\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "D = 6\n",
    "\n",
    "E=2\n",
    "F=3\n",
    "C = (E*F)\n",
    "\n",
    "\n",
    "h=0.001\n",
    "A  = B * C + D\n",
    "Adb = (B+h) * C + D\n",
    "Adc = B * (C+h) + D\n",
    "Add = B * C + (D+h)\n",
    "Ade = B * ((E+h)*F) + D\n",
    "Adf = B * (E*(F+h)) + D\n",
    "Cde = (E+h)*F\n",
    "Cdf = E*(F+h)\n",
    "\n",
    "da_db= (Adb-A)/h\n",
    "Da_db= C\n",
    "da_dc= (Adc-A)/h\n",
    "Da_dc = B\n",
    "da_dd= (Add-A)/h\n",
    "Da_dd=1\n",
    "\n",
    "da_de= (Ade-A)/h\n",
    "da_df= (Adf-A)/h\n",
    "\n",
    "dc_de= (Cde-C)/h\n",
    "Dc_de=F\n",
    "dc_df= (Cdf-C)/h\n",
    "Dc_df=E\n",
    "\n",
    "\n",
    "print(' da_db ',da_db,'\\n da_dc', da_dc, '\\n da_dd', da_dd,'\\n dc_de', dc_de, '\\n dc_df', dc_df, '\\n da_de', da_de, '\\n da_df', da_df)\n",
    "print(f' da_df = da_dc x dc_df -> {da_df} = {da_dc} x {dc_df} \\n da_de = da_dc x dc_de -> {da_de} = {da_dc} x {dc_de}')\n",
    "print(f'COMPOSE FROM VARS\\n da_df =  Da_dc x Dc_df -> {Da_dc*Dc_df} = {Da_dc} x {Dc_df} \\n da_de = Da_dc x Dc_de -> {Da_dc*Dc_de} = {Da_dc} x {Dc_de}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.1073,  0.7434],\n",
       "         [-2.1842, -0.2667]], requires_grad=True),\n",
       " tensor([[0.0000, 0.7434],\n",
       "         [0.0000, 0.0000]], grad_fn=<ReluBackward0>),\n",
       " tensor([[False,  True],\n",
       "         [False, False]]),\n",
       " tensor(0.7434, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.randn(2,2, requires_grad=True)\n",
    "s= torch.relu(r)\n",
    "mask = r>0\n",
    "t=torch.sum(s)\n",
    "t.backward()\n",
    "r,s,mask, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
