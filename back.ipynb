{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical aproximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000001336 5.000000000002558 1.0000000000012221\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "C = 4\n",
    "D = 6\n",
    "h=0.001\n",
    "A  = B * C + D\n",
    "Adb = (B+h) * C + D\n",
    "Adc = B * (C+h) + D\n",
    "Add = B * C + (D+h)\n",
    "da_db= (Adb-A)/h\n",
    "da_dc= (Adc-A)/h\n",
    "da_dd= (Add-A)/h\n",
    "\n",
    "print(da_db, da_dc, da_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " da_db  6.000000000000227 \n",
      " da_dc 5.000000000002558 \n",
      " da_dd 0.9999999999976694 \n",
      " dc_de 3.0000000000001137 \n",
      " dc_df 1.9999999999997797 \n",
      " da_de 15.000000000000568 \n",
      " da_df 9.99999999999801\n",
      " da_df = da_dc x dc_df -> 9.99999999999801 = 5.000000000002558 x 1.9999999999997797 \n",
      " da_de = da_dc x dc_de -> 15.000000000000568 = 5.000000000002558 x 3.0000000000001137\n",
      "COMPOSE FROM VARS\n",
      " da_df =  Da_dc x Dc_df -> 10 = 5 x 2 \n",
      " da_de = Da_dc x Dc_de -> 15 = 5 x 3\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "D = 6\n",
    "\n",
    "E=2\n",
    "F=3\n",
    "C = (E*F)\n",
    "\n",
    "\n",
    "h=0.001\n",
    "A  = B * C + D\n",
    "Adb = (B+h) * C + D\n",
    "Adc = B * (C+h) + D\n",
    "Add = B * C + (D+h)\n",
    "Ade = B * ((E+h)*F) + D\n",
    "Adf = B * (E*(F+h)) + D\n",
    "Cde = (E+h)*F\n",
    "Cdf = E*(F+h)\n",
    "\n",
    "da_db= (Adb-A)/h\n",
    "Da_db= C\n",
    "da_dc= (Adc-A)/h\n",
    "Da_dc = B\n",
    "da_dd= (Add-A)/h\n",
    "Da_dd=1\n",
    "\n",
    "da_de= (Ade-A)/h\n",
    "da_df= (Adf-A)/h\n",
    "\n",
    "dc_de= (Cde-C)/h\n",
    "Dc_de=F\n",
    "dc_df= (Cdf-C)/h\n",
    "Dc_df=E\n",
    "\n",
    "\n",
    "print(' da_db ',da_db,'\\n da_dc', da_dc, '\\n da_dd', da_dd,'\\n dc_de', dc_de, '\\n dc_df', dc_df, '\\n da_de', da_de, '\\n da_df', da_df)\n",
    "print(f' da_df = da_dc x dc_df -> {da_df} = {da_dc} x {dc_df} \\n da_de = da_dc x dc_de -> {da_de} = {da_dc} x {dc_de}')\n",
    "print(f'COMPOSE FROM VARS\\n da_df =  Da_dc x Dc_df -> {Da_dc*Dc_df} = {Da_dc} x {Dc_df} \\n da_de = Da_dc x Dc_de -> {Da_dc*Dc_de} = {Da_dc} x {Dc_de}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten:   \n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.size(0), -1) #keep the batch dimension\n",
    "        # self.out=torch.flatten(x)\n",
    "        self.x=x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.grad= torch.ones_like(self.x)\n",
    "        return self.grad\n",
    "    \n",
    "class simple_sum:\n",
    "    def __init__(self, dim=0, keepdims=True):\n",
    "        self.dim=dim\n",
    "        self.keepdims=keepdims\n",
    "    def __call__(self, x):\n",
    "        self.out = x.sum(dim=self.dim, keepdims=self.keepdims)\n",
    "        self.x=x\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.grad= torch.ones_like(self.x)\n",
    "        return self.grad\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class simple_mul:\n",
    "    def __init__(self):\n",
    "        self.scalar=torch.rand(1, requires_grad=True)\n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out = x*self.scalar\n",
    "        return self.out \n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.scalar_grad= self.x #even though scalar is a single element the dimensions should be the broadcasted element trhough the whole input, ergo must match x and only when updating the parameter should be summed to a sinlge scalar value\n",
    "            self.grad=torch.ones_like(self.x)*self.scalar\n",
    "        return self.grad   \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class simple_relu:\n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out= torch.maximum(x, torch.zeros_like(x))\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            mask=self.x > 0 #alternative mask -> self.out > 0 but in any case if self.x > 0 then self.out > 0 and self.out <= 0 otherwise so both options should lead to the same mask\n",
    "            self.grad=mask*1.0\n",
    "        return self.grad\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class simple_linear:\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        self.w = torch.randn(fan_in, fan_out, requires_grad=True)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out= x @ self.w\n",
    "        return self.out\n",
    "    \n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.w_grad=self.x.T\n",
    "            self.grad=self.w.T\n",
    "        return self.grad\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2260, 0.4255], requires_grad=True)\n",
      "out tensor([1.6516], grad_fn=<SumBackward1>)\n",
      "x.grad None\n",
      "out.grad None\n",
      "x.grad tensor([1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_984/2354017059.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print('out.grad',out.grad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simp=simple_sum()\n",
    "x=torch.randn(2)\n",
    "x.requires_grad=True\n",
    "print(x)\n",
    "out=simp(x)\n",
    "print('out',out)\n",
    "print('x.grad',x.grad)\n",
    "out.backward()\n",
    "\n",
    "print('out.grad',out.grad)\n",
    "print('x.grad',x.grad)\n",
    "\n",
    "simp.back(),simp.x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5175, 0.2790], requires_grad=True)\n",
      "outm tensor([0.0811, 0.0437], grad_fn=<MulBackward0>)\n",
      "y.grad None\n",
      "y.grad tensor([0.1566, 0.1566])\n"
     ]
    }
   ],
   "source": [
    "simpm=simple_mul()\n",
    "y=torch.randn(2)\n",
    "y.requires_grad=True\n",
    "print(y)\n",
    "outm=simpm(y)\n",
    "print('outm',outm)\n",
    "print('y.grad',y.grad)\n",
    "outm_sum=outm.sum(0)\n",
    "outm_sum.backward()\n",
    "# print('outm.grad',outm.grad)\n",
    "print('y.grad',y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1566], requires_grad=True),\n",
       " tensor([0.1566, 0.1566]),\n",
       " tensor([0.1566, 0.1566]),\n",
       " tensor([0.7966]),\n",
       " tensor([0.5175, 0.2790], requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpm.scalar, simpm.back(),simpm.x.grad, simpm.scalar.grad, simpm.scalar_grad,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[ 0.1528,  0.8371],\n",
      "        [ 1.3003, -0.4865]], requires_grad=True) \n",
      "k: tensor([[1.4531, 0.3506]], grad_fn=<SumBackward1>) \n",
      "l: tensor([[1.0905, 0.2631]], grad_fn=<MulBackward0>) \n",
      "m: tensor([1.3536], grad_fn=<SumBackward1>) \n",
      "mul1.scalar: tensor([0.7505], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "sum1=simple_sum(0)\n",
    "mul1=simple_mul()\n",
    "sum2=simple_sum(1, keepdims=False)\n",
    "j=torch.randn(2,2, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=sum1(j)\n",
    "k.retain_grad()\n",
    "l=mul1(k)\n",
    "l.retain_grad()\n",
    "# m=torch.sum(l)\n",
    "m=sum2(l)\n",
    "m.retain_grad()\n",
    "m.backward()\n",
    "print('j:',j, '\\nk:', k , '\\nl:',l,'\\nm:', m,'\\nmul1.scalar:', mul1.scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul1.back() tensor([[0.7505, 0.7505]]) \n",
      "mul1.scalar_grad tensor([[1.4531, 0.3506]], grad_fn=<SumBackward1>) \n",
      "mul1.scalar.grad tensor([1.8037])\n",
      "sum1.back() tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "j.grad tensor([[0.7505, 0.7505],\n",
      "        [0.7505, 0.7505]]) torch.Size([2, 2]) \n",
      "k.grad tensor([[0.7505, 0.7505]]) torch.Size([1, 2]) \n",
      "l.grad tensor([[1., 1.]]) torch.Size([1, 2]) \n",
      "m.grad tensor([1.]) torch.Size([1])\n",
      "mul1.scalar.grad tensor([1.8037])\n",
      "*************** calculated with .back() in each layer********************\n",
      "dm_dl -> l grad  tensor([[1., 1.]])\n",
      "dm_dk = dm_dl*dl_dk = sum2.back() * mul1.back() \n",
      "k grad -> md_dk tensor([[0.7505, 0.7505]]) back.shape torch.Size([1, 2])\n",
      "dm_dj = dm_dl*dl_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()\n",
      "dm_dj = dm_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()\n",
      "j grad -> dm_dj tensor([[0.7505, 0.7505],\n",
      "        [0.7505, 0.7505]]) back.shape torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# dm_dj = dm_dl*dl_dk*dk_dj\n",
    "#dm_dl -> sum2 ->  [1, 1] -> dimensions = l.dimensions\n",
    "#dl_dk -> mul1 -> [mul1.scalar, mul1.scalar] -> dimensions = k.dimensions\n",
    "#dk_dj -> sum1 -> [[1., 1.],\n",
    "#                  [1., 1.]] -> dimensions = j.dimensions\n",
    "#dm_dj = [[mul1.scalar, mul1.scalar],\n",
    "#         [mul1.scalar, mul1.scalar]] -> dimensions = j.dimensions\n",
    "\n",
    "print('mul1.back()',mul1.back(), '\\nmul1.scalar_grad', mul1.scalar_grad, '\\nmul1.scalar.grad', mul1.scalar.grad)\n",
    "print('sum1.back()',sum1.back())\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nm.grad',m.grad, m.shape)\n",
    "print('mul1.scalar.grad',mul1.scalar.grad)\n",
    "print('*************** calculated with .back() in each layer********************')\n",
    "\n",
    "mul1_back=mul1.back() #dl_dk\n",
    "sum1_back=sum1.back() #dk_dj\n",
    "sum2_back=sum2.back() #dm_dl\n",
    "\n",
    "print('dm_dl -> l grad ',sum2_back )\n",
    "print('dm_dk = dm_dl*dl_dk = sum2.back() * mul1.back() ')\n",
    "back = sum2.back() * mul1_back \n",
    "print('k grad -> md_dk',back, 'back.shape',back.shape)\n",
    "back = back *sum1_back\n",
    "print('dm_dj = dm_dl*dl_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()')\n",
    "print('dm_dj = dm_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()')\n",
    "print('j grad -> dm_dj',back, 'back.shape',back.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding  relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[-1.0020,  0.0559],\n",
      "        [-0.9885, -1.4788]], requires_grad=True) \n",
      "k: tensor([[-1.9904, -1.4229]], grad_fn=<SumBackward1>) \n",
      "l: tensor([[-0.0312, -0.0223]], grad_fn=<MulBackward0>) \n",
      "r: tensor([[0., 0.]], grad_fn=<MaximumBackward0>) \n",
      "m: tensor([0.], grad_fn=<SumBackward1>) \n",
      "mul1.scalar: tensor([0.0157], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "rsum1=simple_sum(0)\n",
    "rmul1=simple_mul()\n",
    "relu1=simple_relu()\n",
    "rsum2=simple_sum(1, keepdims=False)\n",
    "\n",
    "j=torch.randn(2,2, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=rsum1(j)\n",
    "k.retain_grad()\n",
    "l=rmul1(k)\n",
    "l.retain_grad()\n",
    "r=relu1(l)\n",
    "r.retain_grad()\n",
    "m=rsum2(r)\n",
    "m.retain_grad()\n",
    "m.backward()\n",
    "print('j:',j, '\\nk:', k , '\\nl:',l, '\\nr:',r,'\\nm:', m,'\\nmul1.scalar:', rmul1.scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***local derivatives***\n",
      "rsum2.back() tensor([[1., 1.]])\n",
      "relu1.back() tensor([[0., 0.]])\n",
      "mul1.back() tensor([[0.0157, 0.0157]])\n",
      "mul1.scalar tensor([[-1.9904, -1.4229]], grad_fn=<SumBackward1>)\n",
      "sum1.back() tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "***global derivatives from autograd***\n",
      "j.grad tensor([[0., 0.],\n",
      "        [0., 0.]]) torch.Size([2, 2]) \n",
      "k.grad tensor([[0., 0.]]) torch.Size([1, 2]) \n",
      "l.grad tensor([[0., 0.]]) torch.Size([1, 2]) \n",
      "r.grad tensor([[1., 1.]]) torch.Size([1, 2]) \n",
      "m.grad tensor([1.]) torch.Size([1])\n",
      "mul1 scalar grad =  tensor([0.])\n",
      "***global derivatives applying chain rule in each layer .back() method***\n",
      "r grad -> dm_dr   tensor([[1., 1.]])\n",
      "*\n",
      "dm_dl = dm_dr*dr_dl = sum2.back() * relu1.back() \n",
      "l grad -> dm_dl tensor([[0., 0.]]) back.shape torch.Size([1, 2])\n",
      "*\n",
      "dm_dk = dm_dr*dr_dl*dl_dk = dm_dl*dl_dk = l grad *mul1.back() \n",
      "k grad -> dm_dk tensor([[0., 0.]]) back.shape torch.Size([1, 2])\n",
      "*\n",
      "dm_dj = dm_dr*dr_dl*dl_dk*dk_dj = dm_dk*dk_dj = k grad * sum1_back \n",
      "j grad -> dm_dj tensor([[0., 0.],\n",
      "        [0., 0.]]) back.shape torch.Size([2, 2])\n",
      "mul1.scalar_grad = dm_dr*dr_dl*dl_dscalar = dm_dl*dl_dscalar = l grad* mul1_sback\n",
      "mul1 scalar grad = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# dm_dj = dm_dr*dr_dl*dl_dk*dk_dj\n",
    "#dm_dr -> sum2 ->  [1, 1] -> dimensions = l.dimensions\n",
    "#dr_dl -> relu1 -> [max(0,l1)>0,max(0,l2)>0] -> dimensions = l.dimensions -> max(0,l2)>0 gives 1 if >0 and 0 if = 0\n",
    "#dl_dk -> mul1 -> [mul1.scalar, mul1.scalar] -> dimensions = k.dimensions\n",
    "#dk_dj -> sum1 -> [[1., 1.],\n",
    "#                  [1., 1.]] -> dimensions = j.dimensions\n",
    "#dm_dj = [[mul1.scalar, mul1.scalar],\n",
    "#         [mul1.scalar, mul1.scalar]] -> dimensions = j.dimensions\n",
    "print('***local derivatives***')\n",
    "print('rsum2.back()',rsum2.back())\n",
    "print('relu1.back()',relu1.back())\n",
    "print('mul1.back()',rmul1.back())\n",
    "print('mul1.scalar',rmul1.scalar_grad)\n",
    "print('sum1.back()',rsum1.back())\n",
    "print('***global derivatives from autograd***')\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nr.grad',r.grad,r.shape,'\\nm.grad',m.grad, m.shape)\n",
    "print('mul1 scalar grad = ', rmul1.scalar.grad)\n",
    "print('***global derivatives applying chain rule in each layer .back() method***')\n",
    "\n",
    "sum1_back=rsum1.back() #dk_dj\n",
    "mul1_back=rmul1.back() #dl_dk\n",
    "relu1_back=relu1.back() #dk_dj\n",
    "sum2_back=rsum2.back() #dm_dl\n",
    "mul1_sback = rmul1.scalar_grad\n",
    "\n",
    "print('r grad -> dm_dr  ',sum2_back )\n",
    "# print('l grad -> dr_dl   ',relu1_back )\n",
    "print('*')\n",
    "print('dm_dl = dm_dr*dr_dl = sum2.back() * relu1.back() ')\n",
    "back1 = sum2_back * relu1_back\n",
    "print('l grad -> dm_dl',back1, 'back.shape',back1.shape)\n",
    "print('*')\n",
    "print('dm_dk = dm_dr*dr_dl*dl_dk = dm_dl*dl_dk = l grad *mul1.back() ')\n",
    "back2 = back1 * mul1_back\n",
    "print('k grad -> dm_dk',back2, 'back.shape',back2.shape)\n",
    "print('*')\n",
    "print('dm_dj = dm_dr*dr_dl*dl_dk*dk_dj = dm_dk*dk_dj = k grad * sum1_back ')\n",
    "back3 = back2 *sum1_back\n",
    "print('j grad -> dm_dj',back3, 'back.shape',back3.shape)\n",
    "\n",
    "print('mul1.scalar_grad = dm_dr*dr_dl*dl_dscalar = dm_dl*dl_dscalar = l grad* mul1_sback')\n",
    "sback = back1 * mul1_sback\n",
    "with torch.no_grad():\n",
    "    sback=sback.sum() #add scalar derivative in each element\n",
    "print('mul1 scalar grad =', sback)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[-0.2845,  0.3976],\n",
      "        [ 0.2680, -0.0267]], requires_grad=True) \n",
      "k: tensor([[ 0.5998, -0.8919],\n",
      "        [-0.4811,  0.2468]], grad_fn=<MmBackward0>) \n",
      " linear w: tensor([[-1.7711,  0.7513],\n",
      "        [ 0.2410, -1.7053]], requires_grad=True) \n",
      "l: tensor([[0.5998, 0.0000],\n",
      "        [0.0000, 0.2468]], grad_fn=<MaximumBackward0>) \n",
      "m: tensor([0.5998, 0.2468], grad_fn=<SumBackward1>) \n",
      "n: tensor(0.8466, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "lin=simple_linear(2,2)\n",
    "relu=simple_relu()\n",
    "sum1=simple_sum(1, keepdims=False)\n",
    "sum2=simple_sum(0, keepdims=False)\n",
    "\n",
    "j=torch.randn(2,2, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=lin(j)\n",
    "k.retain_grad()\n",
    "l=relu(k)\n",
    "l.retain_grad()\n",
    "m=sum1(l)\n",
    "m.retain_grad()\n",
    "n=sum2(m)\n",
    "n.retain_grad()\n",
    "n.backward()\n",
    "print('j:',j, '\\nk:', k , '\\n linear w:',lin.w, '\\nl:',l, '\\nm:', m, '\\nn:', n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***local derivatives***\n",
      "sum2.back() tensor([1., 1.])\n",
      "sum1.back() tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "relu.back() tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "lin.back() tensor([[-1.7711,  0.2410],\n",
      "        [ 0.7513, -1.7053]], requires_grad=True)\n",
      "lin.w tensor([[-1.7711,  0.7513],\n",
      "        [ 0.2410, -1.7053]], requires_grad=True)\n",
      "***global derivatives from autograd***\n",
      "j.grad tensor([[-1.7711,  0.2410],\n",
      "        [ 0.7513, -1.7053]]) torch.Size([2, 2]) \n",
      "k.grad tensor([[1., 0.],\n",
      "        [0., 1.]]) torch.Size([2, 2]) \n",
      "l.grad tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2]) \n",
      "m.grad tensor([1., 1.]) torch.Size([2]) \n",
      "n.grad tensor(1.) torch.Size([])\n",
      "lin.w.grad tensor([[-0.2845,  0.2680],\n",
      "        [ 0.3976, -0.0267]])\n",
      "***global derivatives applying chain rule in each layer .back() method***\n",
      "m grad -> dn_dm   tensor([1., 1.])\n",
      "*\n",
      "dn_dl = dn_dm*dm_dl = sum2.back() * sum1_back \n",
      "l grad -> dn_dl tensor([[1., 1.],\n",
      "        [1., 1.]]) back.shape torch.Size([2, 2])\n",
      "*\n",
      "dn_dk = dn_dm*dm_dl*dl_dk = dn_dl*dl_dk = l grad *relu_back \n",
      "k grad -> dn_dk tensor([[1., 0.],\n",
      "        [0., 1.]]) back.shape torch.Size([2, 2])\n",
      "*\n",
      "dn_dj = dn_dm*dm_dl*dl_dk*dk_dj = dn_dk*dk_dj = k grad * lin_back \n",
      "j grad -> dn_dj tensor([[-1.7711,  0.2410],\n",
      "        [ 0.7513, -1.7053]], grad_fn=<MmBackward0>) back.shape torch.Size([2, 2])\n",
      "w.grad -> dn_dw tensor([[-0.2845,  0.2680],\n",
      "        [ 0.3976, -0.0267]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dn_dj = dn_dm*dm_dl*dl_dk*dk_dj\n",
    "#dk_dj -> rlin -> (2,2)  -> dimensions = k.dimensions\n",
    "#dl_dk -> relu -> (2,2) -> dimensions = l.dimensions \n",
    "#dm_dl -> sum1 -> ()-> dimensions = k.dimensions\n",
    "#dn_dm -> sum2 -> ()-> dimensions = j.dimensions\n",
    "#dn_dj =  -> dimensions = j.dimensions\n",
    "print('***local derivatives***')\n",
    "print('sum2.back()',sum2.back())\n",
    "print('sum1.back()',sum1.back())\n",
    "print('relu.back()',relu.back())\n",
    "print('lin.back()',lin.back())\n",
    "print('lin.w',lin.w)\n",
    "print('***global derivatives from autograd***')\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nm.grad',m.grad,m.shape,'\\nn.grad',n.grad, n.shape)\n",
    "\n",
    "print('lin.w.grad', lin.w.grad)\n",
    "\n",
    "\n",
    "print('***global derivatives applying chain rule in each layer .back() method***')\n",
    "\n",
    "sum2_back=sum2.back() #dn_dm\n",
    "sum1_back=sum1.back() #dm_dl\n",
    "relu_back=relu.back() #dl_dk\n",
    "lin_back=lin.back()  #dk_dj\n",
    "lin_w_back=lin.w_grad #dk_dw\n",
    "\n",
    "print('m grad -> dn_dm  ',sum2_back )\n",
    "print('*')\n",
    "print('dn_dl = dn_dm*dm_dl = sum2.back() * sum1_back ')\n",
    "back1 = sum2_back * sum1_back\n",
    "print('l grad -> dn_dl',back1, 'back.shape',back1.shape)\n",
    "print('*')\n",
    "print('dn_dk = dn_dm*dm_dl*dl_dk = dn_dl*dl_dk = l grad *relu_back ')\n",
    "back2 = back1 * relu_back\n",
    "print('k grad -> dn_dk',back2, 'back.shape',back2.shape)\n",
    "print('*')\n",
    "print('dn_dj = dn_dm*dm_dl*dl_dk*dk_dj = dn_dk*dk_dj = k grad * lin_back ')\n",
    "back3 = back2 @lin_back\n",
    "print('j grad -> dn_dj',back3, 'back.shape',back3.shape)\n",
    "print('w.grad -> dn_dw', lin_w_back@ back2)\n",
    "# print('mul1.scalar_grad = dm_dr*dr_dl*dl_dscalar = dm_dl*dl_dscalar = l grad* lin_back')\n",
    "# sback = back1 * mul1_sback\n",
    "# with torch.no_grad():\n",
    "#     sback=sback.sum() #add scalar derivative in each element\n",
    "# print('mul1 scalar grad =', sback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "root = './MNIST/'\n",
    "dataset = torchvision.datasets.MNIST(root = root, train = True, transform = transforms.ToTensor())\n",
    "x= dataset.__getitem__(1)\n",
    "plt.imshow(x[0].squeeze(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=0.9*len(dataset)\n",
    "val_size=0.1*len(dataset)\n",
    "train_dataset, val_dataset = random_split(dataset, [int(train_size),int(val_size) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back REF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "inner_size=128\n",
    "output_size=64\n",
    "cat_size=10\n",
    "generator=torch.Generator().manual_seed(23)\n",
    "layers = [\n",
    "    flatten(),\n",
    "    simple_linear(input_size,inner_size),\n",
    "    simple_relu(),\n",
    "    simple_linear(inner_size, cat_size)\n",
    "]\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(layers, x): \n",
    "    # print(x.shape)\n",
    "    for layer in layers:\n",
    "        # print(layer.__class__)\n",
    "        # print (x.shape)\n",
    "        x=layer(x)\n",
    "        # print(layer.__class__)\n",
    "        # print(x.shape)    \n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 84.49212646484375\n",
      "step = 1000, loss = 5.160953044891357\n",
      "step = 2000, loss = 3.610795259475708\n",
      "step = 3000, loss = 3.3007068634033203\n",
      "step = 4000, loss = 2.2701773643493652\n",
      "step = 5000, loss = 1.9651966094970703\n",
      "step = 6000, loss = 1.7417259216308594\n",
      "step = 7000, loss = 1.4039407968521118\n",
      "step = 8000, loss = 1.6005581617355347\n",
      "step = 9000, loss = 0.997118353843689\n",
      "step = 10000, loss = 1.183626651763916\n",
      "step = 11000, loss = 0.9781593084335327\n",
      "step = 12000, loss = 0.9751796722412109\n",
      "step = 13000, loss = 0.8659653067588806\n",
      "step = 14000, loss = 0.7256831526756287\n",
      "step = 15000, loss = 0.7801238298416138\n",
      "step = 16000, loss = 0.6224373579025269\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "iteration = 0\n",
    "steps=40000\n",
    "batch_size = 10\n",
    "epochs=3\n",
    "\n",
    "loss_steps=1000\n",
    "\n",
    "steps_data = 500\n",
    "\n",
    "# batching\n",
    "subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i=0\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in subset_loader:\n",
    "\n",
    "        # print (type(batch_x))\n",
    "        # print (type(batch_y))\n",
    "\n",
    "        len_dataset=len(train_dataset)\n",
    "        index = torch.randint(low=1, high=len_dataset, size=(), generator=generator)\n",
    "\n",
    "        row=train_dataset.__getitem__(index)\n",
    "        # out=forward(layers,row[0])\n",
    "        # out=forward(layers,batch_x.squeeze())\n",
    "        \n",
    "        out1 = layers[0](batch_x.squeeze())\n",
    "        out2 = layers[1](out1)\n",
    "        out3 = layers[2](out2)\n",
    "        out = layers[3](out3)\n",
    "\n",
    "\n",
    "\n",
    "        # loss = F.cross_entropy(out.squeeze(),torch.tensor(row[1]))\n",
    "        loss = F.cross_entropy(out.squeeze(),batch_y)\n",
    "        \n",
    "        for layer in layers:\n",
    "            if not isinstance(layer, flatten):\n",
    "                layer.out.retain_grad() \n",
    "                \n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "            \n",
    "        #update parameters\n",
    "        for p in parameters:\n",
    "            p.data += -lr*p.grad\n",
    "        \n",
    "        if i % loss_steps == 0:\n",
    "            with torch.no_grad():\n",
    "                losses=torch.zeros(loss_steps)\n",
    "                for j in range(loss_steps):\n",
    "                    index2 = torch.randint(low=1, high=len_dataset, size=())\n",
    "                    row_loss=train_dataset.__getitem__(index2)\n",
    "                    out_loss = out=forward(layers,row_loss[0])\n",
    "                    losses[j] = F.cross_entropy(out_loss.squeeze(),torch.tensor(row_loss[1]))\n",
    "                print(f'step = {i}, loss = {losses.mean()}')\n",
    "        i+=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "inner_size=128\n",
    "output_size=64\n",
    "cat_size=10\n",
    "generator=torch.Generator().manual_seed(23)\n",
    "layers = [\n",
    "    flatten(),\n",
    "    simple_linear(input_size,inner_size),\n",
    "    simple_relu(),\n",
    "    simple_linear(inner_size, cat_size)\n",
    "]\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:    \n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, loss, lr, out_grad):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        layer_back=out_grad\n",
    "        for layer in reversed(layers):\n",
    "               \n",
    "            if (isinstance(layer, simple_linear) and (layer_back.dim() > 0)):\n",
    "                        \n",
    "                linear_back=layer.back() \n",
    "                w_grad=layer.w_grad@layer_back #weights gradients\n",
    "                \n",
    "                layer_back @=linear_back\n",
    "                \n",
    "                #Update weights\n",
    "                layer.w.data +=-lr*w_grad\n",
    "                continue\n",
    "            if isinstance(layer, flatten):\n",
    "                layer_back =layer_back.view(layer.back().shape)    \n",
    "                continue\n",
    "            \n",
    "            layer_back=layer.back()*layer_back\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 104.70053100585938\n",
      "step = 1000, loss = 5.066598415374756\n",
      "step = 2000, loss = 3.5792078971862793\n",
      "step = 3000, loss = 2.5025882720947266\n",
      "step = 4000, loss = 2.5504744052886963\n",
      "step = 5000, loss = 1.5452630519866943\n",
      "step = 6000, loss = 1.7490812540054321\n",
      "step = 7000, loss = 1.5986084938049316\n",
      "step = 8000, loss = 1.1459687948226929\n",
      "step = 9000, loss = 1.3427740335464478\n",
      "step = 10000, loss = 0.8610731363296509\n",
      "step = 11000, loss = 0.9506800174713135\n",
      "step = 12000, loss = 1.000819206237793\n",
      "step = 13000, loss = 1.0343977212905884\n",
      "step = 14000, loss = 0.9092791080474854\n",
      "step = 15000, loss = 0.8070168495178223\n",
      "step = 16000, loss = 0.6157515645027161\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "iteration = 0\n",
    "steps=40000\n",
    "batch_size = 10\n",
    "epochs=3\n",
    "\n",
    "loss_steps=1000\n",
    "\n",
    "steps_data = 500\n",
    "\n",
    "# batching\n",
    "subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i=0\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in subset_loader:\n",
    "\n",
    "        len_dataset=len(train_dataset)\n",
    "        index = torch.randint(low=1, high=len_dataset, size=(), generator=generator)\n",
    "\n",
    "        row=train_dataset.__getitem__(index)\n",
    "        \n",
    "        out=forward(layers,batch_x.squeeze())\n",
    "        \n",
    "        loss = F.cross_entropy(out.squeeze(),batch_y)\n",
    "        \n",
    "        for layer in layers:\n",
    "            if not isinstance(layer, flatten):\n",
    "                layer.out.retain_grad() \n",
    "        \n",
    "        loss.backward() # only used to compute the loss function gradient and for comparison purposes\n",
    "\n",
    "        backward(layers, loss, lr, out.grad.detach()) #TODO ot.grad.detach() -> replace for actual calculation for DLoss/Dout\n",
    "        \n",
    "        if i % loss_steps == 0:\n",
    "            with torch.no_grad():\n",
    "                losses=torch.zeros(loss_steps)\n",
    "                for j in range(loss_steps):\n",
    "                    index2 = torch.randint(low=1, high=len_dataset, size=())\n",
    "                    row_loss=train_dataset.__getitem__(index2)\n",
    "                    out_loss = out=forward(layers,row_loss[0])\n",
    "                    losses[j] = F.cross_entropy(out_loss.squeeze(),torch.tensor(row_loss[1]))\n",
    "                print(f'step = {i}, loss = {losses.mean()}')\n",
    "        i+=1\n",
    "        # break\n",
    "    # break\n",
    "        \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
