{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical aproximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000001336 5.000000000002558 1.0000000000012221\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "C = 4\n",
    "D = 6\n",
    "h=0.001\n",
    "A  = B * C + D\n",
    "Adb = (B+h) * C + D\n",
    "Adc = B * (C+h) + D\n",
    "Add = B * C + (D+h)\n",
    "da_db= (Adb-A)/h\n",
    "da_dc= (Adc-A)/h\n",
    "da_dd= (Add-A)/h\n",
    "\n",
    "print(da_db, da_dc, da_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " da_db  6.000000000000227 \n",
      " da_dc 5.000000000002558 \n",
      " da_dd 0.9999999999976694 \n",
      " dc_de 3.0000000000001137 \n",
      " dc_df 1.9999999999997797 \n",
      " da_de 15.000000000000568 \n",
      " da_df 9.99999999999801\n",
      " da_df = da_dc x dc_df -> 9.99999999999801 = 5.000000000002558 x 1.9999999999997797 \n",
      " da_de = da_dc x dc_de -> 15.000000000000568 = 5.000000000002558 x 3.0000000000001137\n",
      "COMPOSE FROM VARS\n",
      " da_df =  Da_dc x Dc_df -> 10 = 5 x 2 \n",
      " da_de = Da_dc x Dc_de -> 15 = 5 x 3\n"
     ]
    }
   ],
   "source": [
    "B = 5\n",
    "D = 6\n",
    "\n",
    "E=2\n",
    "F=3\n",
    "C = (E*F)\n",
    "\n",
    "\n",
    "h=0.001\n",
    "A  = B * C + D\n",
    "Adb = (B+h) * C + D\n",
    "Adc = B * (C+h) + D\n",
    "Add = B * C + (D+h)\n",
    "Ade = B * ((E+h)*F) + D\n",
    "Adf = B * (E*(F+h)) + D\n",
    "Cde = (E+h)*F\n",
    "Cdf = E*(F+h)\n",
    "\n",
    "da_db= (Adb-A)/h\n",
    "Da_db= C\n",
    "da_dc= (Adc-A)/h\n",
    "Da_dc = B\n",
    "da_dd= (Add-A)/h\n",
    "Da_dd=1\n",
    "\n",
    "da_de= (Ade-A)/h\n",
    "da_df= (Adf-A)/h\n",
    "\n",
    "dc_de= (Cde-C)/h\n",
    "Dc_de=F\n",
    "dc_df= (Cdf-C)/h\n",
    "Dc_df=E\n",
    "\n",
    "\n",
    "print(' da_db ',da_db,'\\n da_dc', da_dc, '\\n da_dd', da_dd,'\\n dc_de', dc_de, '\\n dc_df', dc_df, '\\n da_de', da_de, '\\n da_df', da_df)\n",
    "print(f' da_df = da_dc x dc_df -> {da_df} = {da_dc} x {dc_df} \\n da_de = da_dc x dc_de -> {da_de} = {da_dc} x {dc_de}')\n",
    "print(f'COMPOSE FROM VARS\\n da_df =  Da_dc x Dc_df -> {Da_dc*Dc_df} = {Da_dc} x {Dc_df} \\n da_de = Da_dc x Dc_de -> {Da_dc*Dc_de} = {Da_dc} x {Dc_de}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flatten:   \n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.size(0), -1) #keep the batch dimension\n",
    "        # self.out=torch.flatten(x)\n",
    "        self.x=x\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.grad= torch.ones_like(self.x)\n",
    "        return self.grad\n",
    "    \n",
    "class simple_sum:\n",
    "    def __init__(self, dim=0, keepdims=True):\n",
    "        self.dim=dim\n",
    "        self.keepdims=keepdims\n",
    "    def __call__(self, x):\n",
    "        self.out = x.sum(dim=self.dim, keepdims=self.keepdims)\n",
    "        self.x=x\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.grad= torch.ones_like(self.x)\n",
    "        return self.grad\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class simple_mul:\n",
    "    def __init__(self):\n",
    "        self.scalar=torch.rand(1, requires_grad=True)\n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out = x*self.scalar\n",
    "        return self.out \n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.scalar_grad= self.x #even though scalar is a single element the dimensions should be the broadcasted element trhough the whole input, ergo must match x and only when updating the parameter should be summed to a sinlge scalar value\n",
    "            self.grad=torch.ones_like(self.x)*self.scalar\n",
    "        return self.grad   \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class simple_relu:\n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out= torch.maximum(x, torch.zeros_like(x))\n",
    "        return self.out\n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            mask=self.x > 0 #alternative mask -> self.out > 0 but in any case if self.x > 0 then self.out > 0 and self.out <= 0 otherwise so both options should lead to the same mask\n",
    "            self.grad=mask*1.0\n",
    "        return self.grad\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class simple_linear:\n",
    "    def __init__(self, fan_in, fan_out, generator):\n",
    "        self.w = torch.randn(fan_in, fan_out, generator=generator, requires_grad=True)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.x=x\n",
    "        self.out= x @ self.w\n",
    "        return self.out\n",
    "    \n",
    "    def back(self):\n",
    "        with torch.no_grad():\n",
    "            self.w_grad=self.x.T\n",
    "            self.grad=self.w.T\n",
    "        return self.grad\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.w] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6351, -0.9510], requires_grad=True)\n",
      "out tensor([0.6841], grad_fn=<SumBackward1>)\n",
      "x.grad None\n",
      "out.grad None\n",
      "x.grad tensor([1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_947/2354017059.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print('out.grad',out.grad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simp=simple_sum()\n",
    "x=torch.randn(2)\n",
    "x.requires_grad=True\n",
    "print(x)\n",
    "out=simp(x)\n",
    "print('out',out)\n",
    "print('x.grad',x.grad)\n",
    "out.backward()\n",
    "\n",
    "print('out.grad',out.grad)\n",
    "print('x.grad',x.grad)\n",
    "\n",
    "simp.back(),simp.x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4480, -0.4927], requires_grad=True)\n",
      "outm tensor([-0.6953, -0.2366], grad_fn=<MulBackward0>)\n",
      "y.grad None\n",
      "y.grad tensor([0.4802, 0.4802])\n"
     ]
    }
   ],
   "source": [
    "simpm=simple_mul()\n",
    "y=torch.randn(2)\n",
    "y.requires_grad=True\n",
    "print(y)\n",
    "outm=simpm(y)\n",
    "print('outm',outm)\n",
    "print('y.grad',y.grad)\n",
    "outm_sum=outm.sum(0)\n",
    "outm_sum.backward()\n",
    "# print('outm.grad',outm.grad)\n",
    "print('y.grad',y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4802], requires_grad=True),\n",
       " tensor([0.4802, 0.4802]),\n",
       " tensor([0.4802, 0.4802]),\n",
       " tensor([-1.9407]),\n",
       " tensor([-1.4480, -0.4927], requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpm.scalar, simpm.back(),simpm.x.grad, simpm.scalar.grad, simpm.scalar_grad,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[ 0.2523,  0.9799],\n",
      "        [ 1.9049, -0.4440]], requires_grad=True) \n",
      "k: tensor([[2.1571, 0.5359]], grad_fn=<SumBackward1>) \n",
      "l: tensor([[1.4459, 0.3592]], grad_fn=<MulBackward0>) \n",
      "m: tensor([1.8051], grad_fn=<SumBackward1>) \n",
      "mul1.scalar: tensor([0.6703], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "sum1=simple_sum(0)\n",
    "mul1=simple_mul()\n",
    "sum2=simple_sum(1, keepdims=False)\n",
    "j=torch.randn(2,2, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=sum1(j)\n",
    "k.retain_grad()\n",
    "l=mul1(k)\n",
    "l.retain_grad()\n",
    "# m=torch.sum(l)\n",
    "m=sum2(l)\n",
    "m.retain_grad()\n",
    "m.backward()\n",
    "print('j:',j, '\\nk:', k , '\\nl:',l,'\\nm:', m,'\\nmul1.scalar:', mul1.scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul1.back() tensor([[0.6703, 0.6703]]) \n",
      "mul1.scalar_grad tensor([[2.1571, 0.5359]], grad_fn=<SumBackward1>) \n",
      "mul1.scalar.grad tensor([2.6930])\n",
      "sum1.back() tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "j.grad tensor([[0.6703, 0.6703],\n",
      "        [0.6703, 0.6703]]) torch.Size([2, 2]) \n",
      "k.grad tensor([[0.6703, 0.6703]]) torch.Size([1, 2]) \n",
      "l.grad tensor([[1., 1.]]) torch.Size([1, 2]) \n",
      "m.grad tensor([1.]) torch.Size([1])\n",
      "mul1.scalar.grad tensor([2.6930])\n",
      "*************** calculated with .back() in each layer********************\n",
      "dm_dl -> l grad  tensor([[1., 1.]])\n",
      "dm_dk = dm_dl*dl_dk = sum2.back() * mul1.back() \n",
      "k grad -> md_dk tensor([[0.6703, 0.6703]]) back.shape torch.Size([1, 2])\n",
      "dm_dj = dm_dl*dl_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()\n",
      "dm_dj = dm_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()\n",
      "j grad -> dm_dj tensor([[0.6703, 0.6703],\n",
      "        [0.6703, 0.6703]]) back.shape torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# dm_dj = dm_dl*dl_dk*dk_dj\n",
    "#dm_dl -> sum2 ->  [1, 1] -> dimensions = l.dimensions\n",
    "#dl_dk -> mul1 -> [mul1.scalar, mul1.scalar] -> dimensions = k.dimensions\n",
    "#dk_dj -> sum1 -> [[1., 1.],\n",
    "#                  [1., 1.]] -> dimensions = j.dimensions\n",
    "#dm_dj = [[mul1.scalar, mul1.scalar],\n",
    "#         [mul1.scalar, mul1.scalar]] -> dimensions = j.dimensions\n",
    "\n",
    "print('mul1.back()',mul1.back(), '\\nmul1.scalar_grad', mul1.scalar_grad, '\\nmul1.scalar.grad', mul1.scalar.grad)\n",
    "print('sum1.back()',sum1.back())\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nm.grad',m.grad, m.shape)\n",
    "print('mul1.scalar.grad',mul1.scalar.grad)\n",
    "print('*************** calculated with .back() in each layer********************')\n",
    "\n",
    "mul1_back=mul1.back() #dl_dk\n",
    "sum1_back=sum1.back() #dk_dj\n",
    "sum2_back=sum2.back() #dm_dl\n",
    "\n",
    "print('dm_dl -> l grad ',sum2_back )\n",
    "print('dm_dk = dm_dl*dl_dk = sum2.back() * mul1.back() ')\n",
    "back = sum2.back() * mul1_back \n",
    "print('k grad -> md_dk',back, 'back.shape',back.shape)\n",
    "back = back *sum1_back\n",
    "print('dm_dj = dm_dl*dl_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()')\n",
    "print('dm_dj = dm_dk*dk_dj = sum2.back() * sum1.back() * mul1.back()')\n",
    "print('j grad -> dm_dj',back, 'back.shape',back.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding  relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[ 0.4599, -0.3250],\n",
      "        [-0.4460,  0.8312]], requires_grad=True) \n",
      "k: tensor([[0.0139, 0.5061]], grad_fn=<SumBackward1>) \n",
      "l: tensor([[0.0120, 0.4363]], grad_fn=<MulBackward0>) \n",
      "r: tensor([[0.0120, 0.4363]], grad_fn=<MaximumBackward0>) \n",
      "m: tensor([0.4483], grad_fn=<SumBackward1>) \n",
      "mul1.scalar: tensor([0.8621], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "rsum1=simple_sum(0)\n",
    "rmul1=simple_mul()\n",
    "relu1=simple_relu()\n",
    "rsum2=simple_sum(1, keepdims=False)\n",
    "\n",
    "j=torch.randn(2,2, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=rsum1(j)\n",
    "k.retain_grad()\n",
    "l=rmul1(k)\n",
    "l.retain_grad()\n",
    "r=relu1(l)\n",
    "r.retain_grad()\n",
    "m=rsum2(r)\n",
    "m.retain_grad()\n",
    "m.backward()\n",
    "print('j:',j, '\\nk:', k , '\\nl:',l, '\\nr:',r,'\\nm:', m,'\\nmul1.scalar:', rmul1.scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***local derivatives***\n",
      "rsum2.back() tensor([[1., 1.]])\n",
      "relu1.back() tensor([[1., 1.]])\n",
      "mul1.back() tensor([[0.8621, 0.8621]])\n",
      "mul1.scalar tensor([[0.0139, 0.5061]], grad_fn=<SumBackward1>)\n",
      "sum1.back() tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "***global derivatives from autograd***\n",
      "j.grad tensor([[0.8621, 0.8621],\n",
      "        [0.8621, 0.8621]]) torch.Size([2, 2]) \n",
      "k.grad tensor([[0.8621, 0.8621]]) torch.Size([1, 2]) \n",
      "l.grad tensor([[1., 1.]]) torch.Size([1, 2]) \n",
      "r.grad tensor([[1., 1.]]) torch.Size([1, 2]) \n",
      "m.grad tensor([1.]) torch.Size([1])\n",
      "mul1 scalar grad =  tensor([0.5200])\n",
      "***global derivatives applying chain rule in each layer .back() method***\n",
      "r grad -> dm_dr   tensor([[1., 1.]])\n",
      "*\n",
      "dm_dl = dm_dr*dr_dl = sum2.back() * relu1.back() \n",
      "l grad -> dm_dl tensor([[1., 1.]]) back.shape torch.Size([1, 2])\n",
      "*\n",
      "dm_dk = dm_dr*dr_dl*dl_dk = dm_dl*dl_dk = l grad *mul1.back() \n",
      "k grad -> dm_dk tensor([[0.8621, 0.8621]]) back.shape torch.Size([1, 2])\n",
      "*\n",
      "dm_dj = dm_dr*dr_dl*dl_dk*dk_dj = dm_dk*dk_dj = k grad * sum1_back \n",
      "j grad -> dm_dj tensor([[0.8621, 0.8621],\n",
      "        [0.8621, 0.8621]]) back.shape torch.Size([2, 2])\n",
      "mul1.scalar_grad = dm_dr*dr_dl*dl_dscalar = dm_dl*dl_dscalar = l grad* mul1_sback\n",
      "mul1 scalar grad = tensor(0.5200)\n"
     ]
    }
   ],
   "source": [
    "# dm_dj = dm_dr*dr_dl*dl_dk*dk_dj\n",
    "#dm_dr -> sum2 ->  [1, 1] -> dimensions = l.dimensions\n",
    "#dr_dl -> relu1 -> [max(0,l1)>0,max(0,l2)>0] -> dimensions = l.dimensions -> max(0,l2)>0 gives 1 if >0 and 0 if = 0\n",
    "#dl_dk -> mul1 -> [mul1.scalar, mul1.scalar] -> dimensions = k.dimensions\n",
    "#dk_dj -> sum1 -> [[1., 1.],\n",
    "#                  [1., 1.]] -> dimensions = j.dimensions\n",
    "#dm_dj = [[mul1.scalar, mul1.scalar],\n",
    "#         [mul1.scalar, mul1.scalar]] -> dimensions = j.dimensions\n",
    "print('***local derivatives***')\n",
    "print('rsum2.back()',rsum2.back())\n",
    "print('relu1.back()',relu1.back())\n",
    "print('mul1.back()',rmul1.back())\n",
    "print('mul1.scalar',rmul1.scalar_grad)\n",
    "print('sum1.back()',rsum1.back())\n",
    "print('***global derivatives from autograd***')\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nr.grad',r.grad,r.shape,'\\nm.grad',m.grad, m.shape)\n",
    "print('mul1 scalar grad = ', rmul1.scalar.grad)\n",
    "print('***global derivatives applying chain rule in each layer .back() method***')\n",
    "\n",
    "sum1_back=rsum1.back() #dk_dj\n",
    "mul1_back=rmul1.back() #dl_dk\n",
    "relu1_back=relu1.back() #dk_dj\n",
    "sum2_back=rsum2.back() #dm_dl\n",
    "mul1_sback = rmul1.scalar_grad\n",
    "\n",
    "print('r grad -> dm_dr  ',sum2_back )\n",
    "# print('l grad -> dr_dl   ',relu1_back )\n",
    "print('*')\n",
    "print('dm_dl = dm_dr*dr_dl = sum2.back() * relu1.back() ')\n",
    "back1 = sum2_back * relu1_back\n",
    "print('l grad -> dm_dl',back1, 'back.shape',back1.shape)\n",
    "print('*')\n",
    "print('dm_dk = dm_dr*dr_dl*dl_dk = dm_dl*dl_dk = l grad *mul1.back() ')\n",
    "back2 = back1 * mul1_back\n",
    "print('k grad -> dm_dk',back2, 'back.shape',back2.shape)\n",
    "print('*')\n",
    "print('dm_dj = dm_dr*dr_dl*dl_dk*dk_dj = dm_dk*dk_dj = k grad * sum1_back ')\n",
    "back3 = back2 *sum1_back\n",
    "print('j grad -> dm_dj',back3, 'back.shape',back3.shape)\n",
    "\n",
    "print('mul1.scalar_grad = dm_dr*dr_dl*dl_dscalar = dm_dl*dl_dscalar = l grad* mul1_sback')\n",
    "sback = back1 * mul1_sback\n",
    "with torch.no_grad():\n",
    "    sback=sback.sum() #add scalar derivative in each element\n",
    "print('mul1 scalar grad =', sback)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j: tensor([[ 1.0550, -0.2669],\n",
      "        [-0.5707, -0.0666]], requires_grad=True) \n",
      "k: tensor([[-0.7915,  0.6709],\n",
      "        [ 0.5308, -0.1976]], grad_fn=<MmBackward0>) \n",
      " linear w: tensor([[-0.8733,  0.4376],\n",
      "        [-0.4866, -0.7840]], requires_grad=True) \n",
      "l: tensor([[0.0000, 0.6709],\n",
      "        [0.5308, 0.0000]], grad_fn=<MaximumBackward0>) \n",
      "m: tensor([0.6709, 0.5308], grad_fn=<SumBackward1>) \n",
      "n: tensor(1.2017, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "generator=torch.Generator().manual_seed(23)\n",
    "lin=simple_linear(2,2, generator=generator)\n",
    "relu=simple_relu()\n",
    "sum1=simple_sum(1, keepdims=False)\n",
    "sum2=simple_sum(0, keepdims=False)\n",
    "\n",
    "j=torch.randn(2,2, requires_grad=True)\n",
    "j.retain_grad()\n",
    "k=lin(j)\n",
    "k.retain_grad()\n",
    "l=relu(k)\n",
    "l.retain_grad()\n",
    "m=sum1(l)\n",
    "m.retain_grad()\n",
    "n=sum2(m)\n",
    "n.retain_grad()\n",
    "n.backward()\n",
    "print('j:',j, '\\nk:', k , '\\n linear w:',lin.w, '\\nl:',l, '\\nm:', m, '\\nn:', n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***local derivatives***\n",
      "sum2.back() tensor([1., 1.])\n",
      "sum1.back() tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "relu.back() tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "lin.back() tensor([[-0.8733, -0.4866],\n",
      "        [ 0.4376, -0.7840]], requires_grad=True)\n",
      "lin.w tensor([[-0.8733,  0.4376],\n",
      "        [-0.4866, -0.7840]], requires_grad=True)\n",
      "***global derivatives from autograd***\n",
      "j.grad tensor([[ 0.4376, -0.7840],\n",
      "        [-0.8733, -0.4866]]) torch.Size([2, 2]) \n",
      "k.grad tensor([[0., 1.],\n",
      "        [1., 0.]]) torch.Size([2, 2]) \n",
      "l.grad tensor([[1., 1.],\n",
      "        [1., 1.]]) torch.Size([2, 2]) \n",
      "m.grad tensor([1., 1.]) torch.Size([2]) \n",
      "n.grad tensor(1.) torch.Size([])\n",
      "lin.w.grad tensor([[-0.5707,  1.0550],\n",
      "        [-0.0666, -0.2669]])\n",
      "***global derivatives applying chain rule in each layer .back() method***\n",
      "m grad -> dn_dm   tensor([1., 1.])\n",
      "*\n",
      "dn_dl = dn_dm*dm_dl = sum2.back() * sum1_back \n",
      "l grad -> dn_dl tensor([[1., 1.],\n",
      "        [1., 1.]]) back.shape torch.Size([2, 2])\n",
      "*\n",
      "dn_dk = dn_dm*dm_dl*dl_dk = dn_dl*dl_dk = l grad *relu_back \n",
      "k grad -> dn_dk tensor([[0., 1.],\n",
      "        [1., 0.]]) back.shape torch.Size([2, 2])\n",
      "*\n",
      "dn_dj = dn_dm*dm_dl*dl_dk*dk_dj = dn_dk*dk_dj = k grad * lin_back \n",
      "j grad -> dn_dj tensor([[ 0.4376, -0.7840],\n",
      "        [-0.8733, -0.4866]], grad_fn=<MmBackward0>) back.shape torch.Size([2, 2])\n",
      "w.grad -> dn_dw tensor([[-0.5707,  1.0550],\n",
      "        [-0.0666, -0.2669]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dn_dj = dn_dm*dm_dl*dl_dk*dk_dj\n",
    "#dk_dj -> rlin -> (2,2)  -> dimensions = k.dimensions\n",
    "#dl_dk -> relu -> (2,2) -> dimensions = l.dimensions \n",
    "#dm_dl -> sum1 -> ()-> dimensions = k.dimensions\n",
    "#dn_dm -> sum2 -> ()-> dimensions = j.dimensions\n",
    "#dn_dj =  -> dimensions = j.dimensions\n",
    "print('***local derivatives***')\n",
    "print('sum2.back()',sum2.back())\n",
    "print('sum1.back()',sum1.back())\n",
    "print('relu.back()',relu.back())\n",
    "print('lin.back()',lin.back())\n",
    "print('lin.w',lin.w)\n",
    "print('***global derivatives from autograd***')\n",
    "print('j.grad', j.grad, j.shape, '\\nk.grad',k.grad,k.shape,'\\nl.grad',l.grad,l.shape,'\\nm.grad',m.grad,m.shape,'\\nn.grad',n.grad, n.shape)\n",
    "\n",
    "print('lin.w.grad', lin.w.grad)\n",
    "\n",
    "\n",
    "print('***global derivatives applying chain rule in each layer .back() method***')\n",
    "\n",
    "sum2_back=sum2.back() #dn_dm\n",
    "sum1_back=sum1.back() #dm_dl\n",
    "relu_back=relu.back() #dl_dk\n",
    "lin_back=lin.back()  #dk_dj\n",
    "lin_w_back=lin.w_grad #dk_dw\n",
    "\n",
    "print('m grad -> dn_dm  ',sum2_back )\n",
    "print('*')\n",
    "print('dn_dl = dn_dm*dm_dl = sum2.back() * sum1_back ')\n",
    "back1 = sum2_back * sum1_back\n",
    "print('l grad -> dn_dl',back1, 'back.shape',back1.shape)\n",
    "print('*')\n",
    "print('dn_dk = dn_dm*dm_dl*dl_dk = dn_dl*dl_dk = l grad *relu_back ')\n",
    "back2 = back1 * relu_back\n",
    "print('k grad -> dn_dk',back2, 'back.shape',back2.shape)\n",
    "print('*')\n",
    "print('dn_dj = dn_dm*dm_dl*dl_dk*dk_dj = dn_dk*dk_dj = k grad * lin_back ')\n",
    "back3 = back2 @lin_back\n",
    "print('j grad -> dn_dj',back3, 'back.shape',back3.shape)\n",
    "print('w.grad -> dn_dw', lin_w_back@ back2)\n",
    "# print('mul1.scalar_grad = dm_dr*dr_dl*dl_dscalar = dm_dl*dl_dscalar = l grad* lin_back')\n",
    "# sback = back1 * mul1_sback\n",
    "# with torch.no_grad():\n",
    "#     sback=sback.sum() #add scalar derivative in each element\n",
    "# print('mul1 scalar grad =', sback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "root = './MNIST/'\n",
    "dataset = torchvision.datasets.MNIST(root = root, train = True, transform = transforms.ToTensor())\n",
    "x= dataset.__getitem__(1)\n",
    "plt.imshow(x[0].squeeze(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=0.9*len(dataset)\n",
    "val_size=0.1*len(dataset)\n",
    "train_dataset, val_dataset = random_split(dataset, [int(train_size),int(val_size) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back REF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "inner_size=128\n",
    "output_size=64\n",
    "cat_size=10\n",
    "generator=torch.Generator().manual_seed(23)\n",
    "layers = [\n",
    "    flatten(),\n",
    "    simple_linear(input_size,inner_size, generator=generator),\n",
    "    simple_relu(),\n",
    "    simple_linear(inner_size, cat_size,generator=generator)\n",
    "]\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(layers, x): \n",
    "    # print(x.shape)\n",
    "    for layer in layers:\n",
    "        x=layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 90.87871551513672\n",
      "step = 1000, loss = 4.206167697906494\n",
      "step = 2000, loss = 3.218306064605713\n",
      "step = 3000, loss = 3.0621256828308105\n",
      "step = 4000, loss = 2.883694648742676\n",
      "step = 5000, loss = 1.7917462587356567\n",
      "step = 6000, loss = 1.8851057291030884\n",
      "step = 7000, loss = 1.4923591613769531\n",
      "step = 8000, loss = 1.9077317714691162\n",
      "step = 9000, loss = 1.2714887857437134\n",
      "step = 10000, loss = 1.1899243593215942\n",
      "step = 11000, loss = 1.0131828784942627\n",
      "step = 12000, loss = 0.9760293960571289\n",
      "step = 13000, loss = 0.6465086936950684\n",
      "step = 14000, loss = 0.9031386971473694\n",
      "step = 15000, loss = 0.7745187878608704\n",
      "step = 16000, loss = 0.7963555455207825\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "iteration = 0\n",
    "steps=40000\n",
    "batch_size = 10\n",
    "epochs=3\n",
    "\n",
    "loss_steps=1000\n",
    "\n",
    "steps_data = 500\n",
    "\n",
    "# batching\n",
    "subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i=0\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in subset_loader:\n",
    "\n",
    "\n",
    "        len_dataset=len(train_dataset)\n",
    "        index = torch.randint(low=1, high=len_dataset, size=(), generator=generator)\n",
    "\n",
    "        row=train_dataset.__getitem__(index)\n",
    "\n",
    "        out=forward(layers,batch_x.squeeze())\n",
    "        \n",
    "        # out1 = layers[0](batch_x.squeeze())\n",
    "        # out2 = layers[1](out1)\n",
    "        # out3 = layers[2](out2)\n",
    "        # out = layers[3](out3)\n",
    "\n",
    "\n",
    "\n",
    "        # loss = F.cross_entropy(out.squeeze(),torch.tensor(row[1]))\n",
    "        loss = F.cross_entropy(out.squeeze(),batch_y)\n",
    "        \n",
    "        for layer in layers:\n",
    "            if not isinstance(layer, flatten):\n",
    "                layer.out.retain_grad() \n",
    "                \n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # print( out1.grad)    \n",
    "        # print(out2.grad.shape, out2.grad)    \n",
    "        # print(out3.grad.shape, out3.grad)    \n",
    "        # print(out.grad.shape, out.grad)    \n",
    "        \n",
    "        \n",
    "        #update parameters\n",
    "        for p in parameters:\n",
    "            p.data += -lr*p.grad\n",
    "        \n",
    "        if i % loss_steps == 0:\n",
    "            with torch.no_grad():\n",
    "                losses=torch.zeros(loss_steps)\n",
    "                for j in range(loss_steps):\n",
    "                    index2 = torch.randint(low=1, high=len_dataset, size=())\n",
    "                    row_loss=train_dataset.__getitem__(index2)\n",
    "                    out_loss = out=forward(layers,row_loss[0])\n",
    "                    losses[j] = F.cross_entropy(out_loss.squeeze(),torch.tensor(row_loss[1]))\n",
    "                print(f'step = {i}, loss = {losses.mean()}')\n",
    "        i+=1\n",
    "    #     break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "inner_size=128\n",
    "output_size=64\n",
    "cat_size=10\n",
    "generator=torch.Generator().manual_seed(23)\n",
    "layers = [\n",
    "    flatten(),\n",
    "    simple_linear(input_size,inner_size, generator=generator),\n",
    "    simple_relu(),\n",
    "    simple_linear(inner_size, cat_size,generator=generator)\n",
    "]\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:    \n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, loss, lr, out_grad):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        layer_back=out_grad\n",
    "        for layer in reversed(layers):\n",
    "               \n",
    "            if (isinstance(layer, simple_linear) and (layer_back.dim() > 0)):\n",
    "                        \n",
    "                linear_back=layer.back() \n",
    "                w_grad=layer.w_grad@layer_back #weights gradients\n",
    "                \n",
    "                # print(layer_back.shape, layer_back)\n",
    "                layer_back @=linear_back\n",
    "                # print(layer_back.shape, layer_back)\n",
    "                \n",
    "                \n",
    "                #Update weights\n",
    "                layer.w.data +=-lr*w_grad\n",
    "                continue\n",
    "            if isinstance(layer, flatten):\n",
    "                layer_back =layer_back.view(layer.back().shape)    \n",
    "                continue\n",
    "            \n",
    "            layer_back=layer.back()*layer_back\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 88.32403564453125\n",
      "step = 1000, loss = 4.905978202819824\n",
      "step = 2000, loss = 3.855635166168213\n",
      "step = 3000, loss = 2.7997183799743652\n",
      "step = 4000, loss = 2.5361340045928955\n",
      "step = 5000, loss = 1.8017687797546387\n",
      "step = 6000, loss = 1.4710537195205688\n",
      "step = 7000, loss = 1.3238192796707153\n",
      "step = 8000, loss = 1.3564379215240479\n",
      "step = 9000, loss = 1.382887363433838\n",
      "step = 10000, loss = 1.340439796447754\n",
      "step = 11000, loss = 1.3557485342025757\n",
      "step = 12000, loss = 0.9448175430297852\n",
      "step = 13000, loss = 0.6130043268203735\n",
      "step = 14000, loss = 0.8625460267066956\n",
      "step = 15000, loss = 0.7904998660087585\n",
      "step = 16000, loss = 0.6841990947723389\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "iteration = 0\n",
    "steps=40000\n",
    "batch_size = 10\n",
    "epochs=3\n",
    "\n",
    "loss_steps=1000\n",
    "\n",
    "steps_data = 500\n",
    "\n",
    "# batching\n",
    "subset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "i=0\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in subset_loader:\n",
    "\n",
    "        len_dataset=len(train_dataset)\n",
    "        index = torch.randint(low=1, high=len_dataset, size=(), generator=generator)\n",
    "\n",
    "        row=train_dataset.__getitem__(index)\n",
    "        \n",
    "        out=forward(layers,batch_x.squeeze())\n",
    "        \n",
    "        loss = F.cross_entropy(out.squeeze(),batch_y)\n",
    "        \n",
    "        for layer in layers:\n",
    "            if not isinstance(layer, flatten):\n",
    "                layer.out.retain_grad() \n",
    "        \n",
    "        loss.backward() # only used to compute the loss function gradient and for comparison purposes\n",
    "\n",
    "        backward(layers, loss, lr, out.grad.detach()) #TODO ot.grad.detach() -> replace for actual calculation for DLoss/Dout\n",
    "        \n",
    "        if i % loss_steps == 0:\n",
    "            with torch.no_grad():\n",
    "                losses=torch.zeros(loss_steps)\n",
    "                for j in range(loss_steps):\n",
    "                    index2 = torch.randint(low=1, high=len_dataset, size=())\n",
    "                    row_loss=train_dataset.__getitem__(index2)\n",
    "                    out_loss = out=forward(layers,row_loss[0])\n",
    "                    losses[j] = F.cross_entropy(out_loss.squeeze(),torch.tensor(row_loss[1]))\n",
    "                print(f'step = {i}, loss = {losses.mean()}')\n",
    "        i+=1\n",
    "    #     break\n",
    "    # break\n",
    "        \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-27.2183, -49.0154,  41.6950,  44.3113, -26.0812,   1.2192, -63.0770,\n",
      "         -23.3116,  36.7450,  13.7780]])\n",
      "<class 'torch.Tensor'>\n",
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x= val_dataset.__getitem__(189)\n",
    "with torch.no_grad():\n",
    "    output =forward(layers,x[0])\n",
    "    print(output)\n",
    "    print(type(output))\n",
    "    print(torch.argmax(F.softmax(output,1)).item())\n",
    "plt.imshow(x[0].squeeze());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
